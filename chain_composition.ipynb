{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "from urllib.error import HTTPError\n",
    "import arxiv\n",
    "from langchain.vectorstores import Qdrant\n",
    "from langchain.document_loaders import PyPDFLoader\n",
    "from langchain.document_loaders import DirectoryLoader\n",
    "from langchain.chat_models import ChatOllama\n",
    "from langchain.embeddings import GPT4AllEmbeddings\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain.pydantic_v1 import BaseModel\n",
    "from langchain.schema.output_parser import StrOutputParser\n",
    "from langchain.schema.runnable import RunnableParallel, RunnablePassthrough\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "en este caso se utiliza la clase arxiv para busqueda de la informacion especifica de su repositorio para guardarlo en local. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dirpath = \"arxiv_papers\"\n",
    "if not os.path.exists(dirpath):\n",
    "    os.makedirs(dirpath)\n",
    "search = arxiv.Search(\n",
    "    query = \"LLM\", # your query length is limited by ARXIV_MAX_QUERY_LENGTH which\n",
    "    max_results = 10,\n",
    "    sort_by = arxiv.SortCriterion.LastUpdatedDate, # you can also use SubmittedDa\n",
    "    sort_order = arxiv.SortOrder.Descending\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Si se encuentra resultados a la busqueda anterior se descarga en formato pdf y lo guarda en la carpeta creada."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for result in search.results():\n",
    "    while True:\n",
    "        try:\n",
    "            result.download_pdf(dirpath=dirpath)\n",
    "            print(f\"-> Paper id {result.get_short_id()} with title '{result.title}\")\n",
    "            break\n",
    "        except FileNotFoundError:\n",
    "            print(\"File not found\")\n",
    "            break\n",
    "        except HTTPError:\n",
    "            print(\"Forbidden\")\n",
    "            break\n",
    "        except ConnectionResetError as e:\n",
    "            print(\"Connection reset by peer\")\n",
    "            time.sleep(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Carga los datos guardados en la carpeta creada anteriormente y concatena toda su informacion en un mismo string y elimina las lineas vacias.\n",
    "Luego se utiliza RecursiveCharacterTextSplitter para dividir todo el texto en chunks mas pequeños"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "papers = []\n",
    "loader = DirectoryLoader(dirpath, glob=\"./*.pdf\", loader_cls=PyPDFLoader)\n",
    "papers = loader.load()\n",
    "print(\"Total number of pages loaded:\", len(papers)) # Total number of pages loa\n",
    "full_text = ''\n",
    "for paper in papers:\n",
    "    full_text = full_text + paper.page_content\n",
    "\n",
    "full_text = \" \".join(l for l in full_text.splitlines() if l)\n",
    "print(len(full_text)) # 1466859\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size = 500,\n",
    "    chunk_overlap = 50\n",
    ")\n",
    "\n",
    "paper_chunks = text_splitter.create_documents([full_text])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Add to vectorDB"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se instancia Qdrant como base de datos Vectorial que guardará los chunks con el formato que le da GPT4AllEmbeddings, guardandolo en una carpeta temporal y seleccionando la coleccion a la que pertenece"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "qdrant = Qdrant.from_documents(\n",
    "    documents=paper_chunks,\n",
    "    embedding=GPT4AllEmbeddings(),\n",
    "    path=\"./tmp/local_qdrant\",\n",
    "    collection_name=\"arxiv_papers\",\n",
    ")\n",
    "\n",
    "retriever = qdrant.as_retriever()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Optionally, pull from the Hub\n",
    "# from langchain import hub\n",
    "# prompt = hub.pull(\"rlm/rag-prompt\")\n",
    "# Or, define your own:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creamos la plantilla que utilizará el modelo añadiendole el contexto y la pregunta del usuario"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "template = \"\"\"Answer the question based only on the following context:\n",
    "{context}\n",
    "Question: {question}\n",
    "\"\"\"\n",
    "prompt = ChatPromptTemplate.from_template(template)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LLM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Seleccionamos un LLm (en este caso se utiliza llama2 pero deve estar descargado en Ollama para poder utilizarlo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select the LLM that you downloaded\n",
    "ollama_llm = \"llama2:7b-chat\"\n",
    "model = ChatOllama(model=ollama_llm)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RAG chain"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Este código define una canalización de procesamiento utilizando la biblioteca LangChain. Analicemos qué hace cada componente de la tubería:\n",
    "\n",
    "EjecutableParalelo :\n",
    "\n",
    "    -Este componente toma dos entradas: \"contexto\" y \"pregunta\".\n",
    "    -Se espera que la entrada de \"contexto\" la proporcione un recuperador (que no se muestra en el fragmento de código).\n",
    "    -La entrada de \"pregunta\" se pasa utilizando el RunnablePassthrough()componente, que simplemente pasa la entrada sin ningún procesamiento.\n",
    "    -El propósito del RunnableParallelcomponente es ejecutar la recuperación del contexto y el procesamiento de la pregunta en paralelo, mejorando potencialmente el rendimiento general del proceso.\n",
    "\n",
    "Prompt :\n",
    "\n",
    "    -Este componente utiliza el ChatPromptTemplateque se definió anteriormente en el código.\n",
    "    -La plantilla de mensaje se utiliza para formatear el contexto de entrada y la pregunta en un mensaje que pueda ser utilizado por el modelo de lenguaje.\n",
    "\n",
    "modelo :\n",
    "    -Este componente representa el modelo de lenguaje que se utilizará para generar la respuesta.\n",
    "    -En este caso, el modelo de lenguaje es un modelo ChatOllama \n",
    "\n",
    "StrOutputParser() :\n",
    "\n",
    "    -Este componente es responsable de analizar la salida del modelo de lenguaje y convertirla en una cadena.\n",
    "    \n",
    "    -El propósito de este componente es garantizar que la salida final de la canalización sea una cadena, que el resto de la aplicación pueda manejar más fácilmente.\n",
    "    \n",
    "    -El propósito general de esta canalización es tomar un contexto y una pregunta como entrada, usar un modelo de lenguaje para generar una respuesta basada en el contexto proporcionado y devolver la respuesta como una cadena. El RunnableParallelcomponente se utiliza para ejecutar la recuperación del contexto y el procesamiento de la pregunta en paralelo, lo que potencialmente mejora el rendimiento del proceso."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "chain = (\n",
    "    RunnableParallel({\"context\": retriever, \"question\": RunnablePassthrough()})\n",
    "    | prompt\n",
    "    | model\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Add typing for input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Question(BaseModel):\n",
    "    __root__: str\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chain = chain.with_types(input_type=Question)\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
